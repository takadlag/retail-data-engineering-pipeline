# Data Ingestion

## ğŸ“Œ Purpose
This folder contains all logic and documentation related to **data ingestion**, which is the process of bringing raw data from external sources into the data platform.

In this project, ingestion is designed to mirror how real-world data engineering teams ingest data from operational systems.

---

## ğŸ“Š Data Source
- **Type:** Public retail transactions dataset
- **Format:** CSV
- **Nature:** Historical transactional data
- **Granularity:** One row per transaction line item

The dataset represents typical retail sales data generated by an e-commerce or point-of-sale system.

---

## â±ï¸ Ingestion Strategy
- **Ingestion Type:** Batch ingestion
- **Frequency:** Periodic (e.g., daily or monthly snapshots)
- **Method:** File-based ingestion

Batch ingestion is chosen because:
- The dataset is historical
- Data does not arrive in real time
- Batch processing is simpler and cost-effective

---

## ğŸ§± Ingestion Design (Conceptual)
1. Raw CSV files are received from the source system
2. Files are stored **as-is** in a raw storage layer
3. No transformations are applied during ingestion
4. Data quality checks happen in later stages

This separation ensures ingestion remains simple and reliable.

---

## ğŸš« What Ingestion Does NOT Do
- No data cleaning
- No business logic
- No aggregations

These steps are intentionally deferred to the transformation layer.

---

## ğŸ“– Notes
- Only small sample files (if any) are stored in this repository
- Full datasets are assumed to live in external storage
- This folder will evolve to include ingestion scripts in later phases
